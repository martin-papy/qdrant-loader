name: Test and Coverage

on:
  push:
    branches: [ main, develop, feature/*, bugfix/*, release/* ]
  pull_request:
    branches: [ main, develop, feature/*, bugfix/*, release/* ]

permissions:
  contents: read
  actions: read

concurrency:
  group: "test-${{ github.ref }}"
  cancel-in-progress: true

jobs:
  test-loader:
    name: Test QDrant Loader
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.12'

      - name: Install system dependencies
        run: |
          # Install ffmpeg for full MarkItDown audio processing capabilities
          # This ensures comprehensive file conversion testing including audio files
          sudo apt-get update
          sudo apt-get install -y ffmpeg

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -e packages/qdrant-loader[dev]

      - name: Create .env.test file for loader
        run: |
          cd packages/qdrant-loader
          cp tests/.env.test.template tests/.env.test
          
          # Check if required secrets are set
          if [ -z "${{ secrets.QDRANT_URL }}" ]; then
            echo "Error: QDRANT_URL secret is not set"
            exit 1
          fi
          if [ -z "${{ secrets.QDRANT_API_KEY }}" ]; then
            echo "Error: QDRANT_API_KEY secret is not set"
            exit 1
          fi
          if [ -z "${{ secrets.QDRANT_COLLECTION_NAME }}" ]; then
            echo "Error: QDRANT_COLLECTION_NAME secret is not set"
            exit 1
          fi
          if [ -z "${{ secrets.OPENAI_API_KEY }}" ]; then
            echo "Error: OPENAI_API_KEY secret is not set"
            exit 1
          fi
          
          # Replace environment variables with proper escaping
          sed -i "s|QDRANT_URL=.*|QDRANT_URL=${{ secrets.QDRANT_URL }}|g" tests/.env.test
          sed -i "s|QDRANT_API_KEY=.*|QDRANT_API_KEY=${{ secrets.QDRANT_API_KEY }}|g" tests/.env.test
          sed -i "s|QDRANT_COLLECTION_NAME=.*|QDRANT_COLLECTION_NAME=${{ secrets.QDRANT_COLLECTION_NAME }}|g" tests/.env.test
          sed -i "s|OPENAI_API_KEY=.*|OPENAI_API_KEY=${{ secrets.OPENAI_API_KEY }}|g" tests/.env.test
          sed -i "s|STATE_DB_PATH=.*|STATE_DB_PATH=:memory:|g" tests/.env.test
          
          # Optional secrets - only replace if they exist
          if [ -n "${{ secrets.REPO_TOKEN }}" ]; then
            sed -i "s|REPO_TOKEN=.*|REPO_TOKEN=${{ secrets.REPO_TOKEN }}|g" tests/.env.test
          fi
          if [ -n "${{ secrets.REPO_URL }}" ]; then
            sed -i "s|REPO_URL=.*|REPO_URL=${{ secrets.REPO_URL }}|g" tests/.env.test
          fi
          if [ -n "${{ secrets.CONFLUENCE_TOKEN }}" ]; then
            sed -i "s|CONFLUENCE_TOKEN=.*|CONFLUENCE_TOKEN=${{ secrets.CONFLUENCE_TOKEN }}|g" tests/.env.test
          fi
          if [ -n "${{ secrets.CONFLUENCE_EMAIL }}" ]; then
            sed -i "s|CONFLUENCE_EMAIL=.*|CONFLUENCE_EMAIL=${{ secrets.CONFLUENCE_EMAIL }}|g" tests/.env.test
          fi
          if [ -n "${{ secrets.CONFLUENCE_URL }}" ]; then
            sed -i "s|CONFLUENCE_URL=.*|CONFLUENCE_URL=${{ secrets.CONFLUENCE_URL }}|g" tests/.env.test
          fi
          if [ -n "${{ secrets.CONFLUENCE_SPACE_KEY }}" ]; then
            sed -i "s|CONFLUENCE_SPACE_KEY=.*|CONFLUENCE_SPACE_KEY=${{ secrets.CONFLUENCE_SPACE_KEY }}|g" tests/.env.test
          fi
          if [ -n "${{ secrets.JIRA_TOKEN }}" ]; then
            sed -i "s|JIRA_TOKEN=.*|JIRA_TOKEN=${{ secrets.JIRA_TOKEN }}|g" tests/.env.test
          fi
          if [ -n "${{ secrets.JIRA_EMAIL }}" ]; then
            sed -i "s|JIRA_EMAIL=.*|JIRA_EMAIL=${{ secrets.JIRA_EMAIL }}|g" tests/.env.test
          fi
          if [ -n "${{ secrets.JIRA_URL }}" ]; then
            sed -i "s|JIRA_URL=.*|JIRA_URL=${{ secrets.JIRA_URL }}|g" tests/.env.test
          fi
          if [ -n "${{ secrets.JIRA_PROJECT_KEY }}" ]; then
            sed -i "s|JIRA_PROJECT_KEY=.*|JIRA_PROJECT_KEY=${{ secrets.JIRA_PROJECT_KEY }}|g" tests/.env.test
          fi
          
          echo "Created .env.test file successfully"
          echo "Contents (with secrets masked):"
          sed 's/=.*/=***/' tests/.env.test

      - name: Create config.test.yaml file for loader
        run: |
          cd packages/qdrant-loader
          cp tests/config.test.template.yaml tests/config.test.yaml
          
          # Replace environment variables in YAML config
          sed -i "s|\${QDRANT_URL}|${{ secrets.QDRANT_URL }}|g" tests/config.test.yaml
          sed -i "s|\${QDRANT_API_KEY}|${{ secrets.QDRANT_API_KEY }}|g" tests/config.test.yaml
          sed -i "s|\${QDRANT_COLLECTION_NAME}|${{ secrets.QDRANT_COLLECTION_NAME }}|g" tests/config.test.yaml
          
          # Optional environment variables - only replace if they exist
          if [ -n "${{ secrets.REPO_TOKEN }}" ]; then
            sed -i "s|\${REPO_TOKEN}|${{ secrets.REPO_TOKEN }}|g" tests/config.test.yaml
          fi
          if [ -n "${{ secrets.REPO_URL }}" ]; then
            sed -i "s|\${REPO_URL}|${{ secrets.REPO_URL }}|g" tests/config.test.yaml
          fi
          if [ -n "${{ secrets.CONFLUENCE_TOKEN }}" ]; then
            sed -i "s|\${CONFLUENCE_TOKEN}|${{ secrets.CONFLUENCE_TOKEN }}|g" tests/config.test.yaml
          fi
          if [ -n "${{ secrets.CONFLUENCE_EMAIL }}" ]; then
            sed -i "s|\${CONFLUENCE_EMAIL}|${{ secrets.CONFLUENCE_EMAIL }}|g" tests/config.test.yaml
          fi
          if [ -n "${{ secrets.CONFLUENCE_URL }}" ]; then
            sed -i "s|\${CONFLUENCE_URL}|${{ secrets.CONFLUENCE_URL }}|g" tests/config.test.yaml
          fi
          if [ -n "${{ secrets.CONFLUENCE_SPACE_KEY }}" ]; then
            sed -i "s|\${CONFLUENCE_SPACE_KEY}|${{ secrets.CONFLUENCE_SPACE_KEY }}|g" tests/config.test.yaml
          fi
          if [ -n "${{ secrets.JIRA_TOKEN }}" ]; then
            sed -i "s|\${JIRA_TOKEN}|${{ secrets.JIRA_TOKEN }}|g" tests/config.test.yaml
          fi
          if [ -n "${{ secrets.JIRA_EMAIL }}" ]; then
            sed -i "s|\${JIRA_EMAIL}|${{ secrets.JIRA_EMAIL }}|g" tests/config.test.yaml
          fi
          if [ -n "${{ secrets.JIRA_URL }}" ]; then
            sed -i "s|\${JIRA_URL}|${{ secrets.JIRA_URL }}|g" tests/config.test.yaml
          fi
          if [ -n "${{ secrets.JIRA_PROJECT_KEY }}" ]; then
            sed -i "s|\${JIRA_PROJECT_KEY}|${{ secrets.JIRA_PROJECT_KEY }}|g" tests/config.test.yaml
          fi
          
          echo "Created config.test.yaml file successfully"
          echo "YAML config structure:"
          head -20 tests/config.test.yaml

      - name: Run loader tests and generate coverage reports
        run: |
          cd packages/qdrant-loader
          python -m pytest tests/ --cov=src --cov-report=xml:../../coverage-loader.xml --cov-report=html:../../htmlcov-loader -v

      - name: Upload loader coverage artifact
        uses: actions/upload-artifact@v4
        with:
          name: coverage-loader-${{ github.run_id }}
          path: |
            htmlcov-loader
            coverage-loader.xml
          retention-days: 30

  test-mcp-server:
    name: Test MCP Server
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.12'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -e packages/qdrant-loader[dev]
          pip install -e packages/qdrant-loader-mcp-server[dev]

      - name: Run MCP server tests and generate coverage reports
        run: |
          cd packages/qdrant-loader-mcp-server
          python -m pytest tests/ --cov=src --cov-report=xml:../../coverage-mcp.xml --cov-report=html:../../htmlcov-mcp -v
        env:
          QDRANT_URL: ${{ secrets.QDRANT_URL }}
          QDRANT_API_KEY: ${{ secrets.QDRANT_API_KEY }}
          QDRANT_COLLECTION_NAME: ${{ secrets.QDRANT_COLLECTION_NAME }}
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}

      - name: Upload MCP server coverage artifact
        uses: actions/upload-artifact@v4
        with:
          name: coverage-mcp-${{ github.run_id }}
          path: |
            htmlcov-mcp
            coverage-mcp.xml
          retention-days: 30

  test-website:
    name: Test Website Build System
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.12'

      - name: Install system dependencies for website testing
        run: |
          # Install system dependencies that might be needed for favicon generation
          sudo apt-get update
          sudo apt-get install -y libcairo2-dev libgirepository1.0-dev

      - name: Install website test dependencies
        run: |
          python -m pip install --upgrade pip
          # Install core dependencies
          pip install pytest pytest-cov pytest-mock
          # Install optional dependencies for comprehensive testing
          pip install markdown pygments tomli
          # Install favicon generation dependencies (optional)
          pip install cairosvg pillow || echo "Optional favicon dependencies not available"

      - name: Run website tests with coverage
        run: |
          # Run all website tests with coverage
          python -m pytest tests/ --cov=website --cov-report=xml:coverage-website.xml --cov-report=html:htmlcov-website --cov-report=term-missing -v
          
          # Display coverage summary
          echo "=== Website Test Coverage Summary ==="
          python -m pytest tests/ --cov=website --cov-report=term --quiet

      - name: Verify website test coverage target
        run: |
          # Check if we achieved our 90% coverage target
          python -c "
          import xml.etree.ElementTree as ET
          tree = ET.parse('coverage-website.xml')
          root = tree.getroot()
          coverage = float(root.attrib['line-rate']) * 100
          print(f'Website test coverage: {coverage:.1f}%')
          if coverage >= 90:
              print('✅ Coverage target achieved (≥90%)')
          else:
              print('❌ Coverage target not met (<90%)')
              exit(1)
          "

      - name: Test website build functionality
        run: |
          # Test that the website can actually be built
          echo "Testing website build functionality..."
          
          # Use real coverage fixtures for testing
          if [ -d "tests/fixtures/coverage-loader/htmlcov-loader" ] && [ -d "tests/fixtures/coverage-mcp/htmlcov-mcp" ]; then
            echo "Using real coverage fixtures for testing..."
            
            # Create the expected structure for the build system
            mkdir -p test-artifacts
            cp -r tests/fixtures/coverage-loader/htmlcov-loader test-artifacts/
            cp -r tests/fixtures/coverage-mcp/htmlcov-mcp test-artifacts/
            
            # Create mock test results
            mkdir -p test-artifacts/test-results
            echo '{"status": "passed", "tests": 42}' > test-artifacts/test-results/status.json
            
            # Test website build with real coverage data
            python website/build.py --output test-site --coverage-artifacts test-artifacts --test-results test-artifacts/test-results
          else
            echo "Using mock data for testing..."
            # Create mock test data as fallback
            mkdir -p test-artifacts/htmlcov-loader test-artifacts/test-results
            echo '{"coverage": 95.5}' > test-artifacts/htmlcov-loader/status.json
            echo '{"status": "passed", "tests": 42}' > test-artifacts/test-results/status.json
            
            # Test website build with mock data
            python website/build.py --output test-site --coverage-artifacts test-artifacts --test-results test-artifacts/test-results
          fi
          
          # Verify build output
          echo "Verifying website build output..."
          test -f test-site/index.html || (echo "❌ Missing index.html" && exit 1)
          test -f test-site/docs/index.html || (echo "❌ Missing docs/index.html" && exit 1)
          test -f test-site/coverage/index.html || (echo "❌ Missing coverage/index.html" && exit 1)
          test -f test-site/sitemap.xml || (echo "❌ Missing sitemap.xml" && exit 1)
          test -f test-site/robots.txt || (echo "❌ Missing robots.txt" && exit 1)
          
          # Verify coverage pages were built with real data
          if [ -f test-site/coverage/loader/index.html ]; then
            echo "✅ Loader coverage page built successfully"
          fi
          if [ -f test-site/coverage/mcp/index.html ]; then
            echo "✅ MCP coverage page built successfully"
          fi
          
          # Count generated files for verification
          html_files=$(find test-site -name "*.html" | wc -l)
          total_files=$(find test-site -type f | wc -l)
          echo "Generated $html_files HTML files and $total_files total files"
          
          echo "✅ Website build test completed successfully"

      - name: Upload website test coverage artifact
        uses: actions/upload-artifact@v4
        with:
          name: coverage-website-${{ github.run_id }}
          path: |
            htmlcov-website
            coverage-website.xml
          retention-days: 30

      - name: Upload website build test artifact
        uses: actions/upload-artifact@v4
        with:
          name: website-build-test-${{ github.run_id }}
          path: test-site/
          retention-days: 7

  test-summary:
    name: Test Summary
    runs-on: ubuntu-latest
    needs: [test-loader, test-mcp-server, test-website]
    if: always()
    steps:
      - name: Check test results
        run: |
          echo "=== Test Results Summary ==="
          echo "QDrant Loader Tests: ${{ needs.test-loader.result }}"
          echo "MCP Server Tests: ${{ needs.test-mcp-server.result }}"
          echo "Website Build Tests: ${{ needs.test-website.result }}"
          
          if [ "${{ needs.test-loader.result }}" != "success" ] || [ "${{ needs.test-mcp-server.result }}" != "success" ] || [ "${{ needs.test-website.result }}" != "success" ]; then
            echo "❌ Some tests failed"
            exit 1
          else
            echo "✅ All tests passed"
          fi

      - name: Create test status artifact
        run: |
          mkdir -p test-results
          echo "{
            \"loader_status\": \"${{ needs.test-loader.result }}\",
            \"mcp_status\": \"${{ needs.test-mcp-server.result }}\",
            \"website_status\": \"${{ needs.test-website.result }}\",
            \"overall_status\": \"${{ (needs.test-loader.result == 'success' && needs.test-mcp-server.result == 'success' && needs.test-website.result == 'success') && 'success' || 'failure' }}\",
            \"run_id\": \"${{ github.run_id }}\",
            \"commit_sha\": \"${{ github.sha }}\",
            \"branch\": \"${{ github.ref_name }}\",
            \"timestamp\": \"$(date -u +%Y-%m-%dT%H:%M:%SZ)\"
          }" > test-results/status.json
          
          echo "Test run completed at $(date)" > test-results/summary.txt
          echo "Run ID: ${{ github.run_id }}" >> test-results/summary.txt
          echo "Commit: ${{ github.sha }}" >> test-results/summary.txt
          echo "Branch: ${{ github.ref_name }}" >> test-results/summary.txt
          echo "Loader Tests: ${{ needs.test-loader.result }}" >> test-results/summary.txt
          echo "MCP Server Tests: ${{ needs.test-mcp-server.result }}" >> test-results/summary.txt
          echo "Website Tests: ${{ needs.test-website.result }}" >> test-results/summary.txt

      - name: Upload test status artifact
        uses: actions/upload-artifact@v4
        with:
          name: test-status-${{ github.run_id }}
          path: test-results/
          retention-days: 30
